{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.0 - Mode Modeling (Exploratory)\n",
    "\n",
    "**Important**: The labels used here are heuristic/pseudo-labels derived from simple rules. This notebook is for exploratory, reproducible baselining and should not be treated as production-grade modeling without proper ground truth and validation.\n",
    "\n",
    "This notebook:\n",
    "- Prepares a trip-level dataset with engineered features\n",
    "- Splits data with temporal awareness (if timestamps exist) or stratified random otherwise\n",
    "- Trains a simple baseline classifier (StandardScaler + LogisticRegression)\n",
    "- Evaluates on a held-out test set\n",
    "- Saves model and reports/figures to the outputs folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from src import modeling\n",
    "\n",
    "OUTPUTS_DIR = \"outputs\"\n",
    "FIG_PATH = os.path.join(OUTPUTS_DIR, \"figures\", \"confusion_matrix_mode_model.png\")\n",
    "VAL_METRICS_JSON = os.path.join(OUTPUTS_DIR, \"reports\", \"model_val_metrics.json\")\n",
    "TEST_REPORT_CSV = os.path.join(OUTPUTS_DIR, \"reports\", \"model_test_classification_report.csv\")\n",
    "MODEL_PATH = os.path.join(OUTPUTS_DIR, \"models\", \"mode_baseline.joblib\")\n",
    "\n",
    "os.makedirs(os.path.join(OUTPUTS_DIR, \"figures\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(OUTPUTS_DIR, \"reports\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(OUTPUTS_DIR, \"models\"), exist_ok=True)\n",
    "\n",
    "print(\"Outputs will be saved under:\", OUTPUTS_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Prepare dataset\n",
    "This will load processed trips and either join heuristic labels if available or compute them using the same thresholds as `src/mode_inference.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_df = modeling.prepare_trip_dataset(\n",
    "    trips_path=\"data/processed/02_trips.parquet\",\n",
    "    heuristic_path=\"data/processed/06_trip_modes_heuristic.parquet\",\n",
    ")\n",
    "trips_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Make splits\n",
    "Temporal leakage avoidance if timestamps exist; otherwise stratified random split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df, test_df = modeling.make_splits(trips_df, test_size=0.2, val_size=0.2, random_state=42)\n",
    "print(\"Train:\", train_df.shape, \", Val:\", val_df.shape, \", Test:\", test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Train baseline classifier\n",
    "StandardScaler + LogisticRegression; also computes a rule-based speed-only baseline for comparison on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline features only\n",
    "model_baseline, val_metrics_baseline = modeling.train_baseline_classifier(train_df, val_df, feature_subset=\"baseline\")\n",
    "print(\"Validation metrics (baseline):\", {k: val_metrics_baseline[k] for k in [\"val_accuracy\", \"val_macro_f1\"]})\n",
    "\n",
    "# Extended features (if available in dataset)\n",
    "model_extended, val_metrics_extended = modeling.train_baseline_classifier(train_df, val_df, feature_subset=\"extended\")\n",
    "print(\"Validation metrics (extended):\", {k: val_metrics_extended[k] for k in [\"val_accuracy\", \"val_macro_f1\"]})\n",
    "\n",
    "# Choose which to proceed with for test evaluation; here we use extended by default\n",
    "model = model_extended\n",
    "val_metrics = val_metrics_extended\n",
    "\n",
    "# Save validation artifacts\n",
    "modeling.save_artifacts(model, val_metrics, outputs_dir=OUTPUTS_DIR)\n",
    "print(\"Saved model to:\", MODEL_PATH)\n",
    "print(\"Saved validation metrics to:\", VAL_METRICS_JSON)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Evaluate on test set and save reports/figures\n",
    "Generates a classification report CSV and confusion matrix figure, plus calibration curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics = modeling.evaluate(model, test_df, outputs_dir=OUTPUTS_DIR, fig_name=\"confusion_matrix_mode_model.png\")\n",
    "print(\"Test metrics (summary):\", {k: test_metrics[k] for k in [\"test_accuracy\", \"test_macro_f1\"]})\n",
    "print(\"Saved classification report CSV to:\", test_metrics[\"classification_report_csv\"]) \n",
    "print(\"Saved confusion matrix figure to:\", test_metrics[\"confusion_matrix_fig\"]) \n",
    "print(\"Extended metrics JSON:\", test_metrics.get(\"metrics_extended_json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4b) Calibration plot display (placeholder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "cal_fig_path = os.path.join(OUTPUTS_DIR, \"figures\", \"mode_model_calibration.png\")\n",
    "if os.path.exists(cal_fig_path):\n",
    "    display(Image(filename=cal_fig_path))\n",
    "else:\n",
    "    print(\"Calibration figure not found yet:\", cal_fig_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Notes\n",
    "- Labels are heuristic/pseudo-labels; treat results as exploratory.\n",
    "- Avoid temporal leakage by using temporal splits and computing features strictly within trip windows.\n",
    "- Improve by adding more realistic features and validated labels, hyperparameter tuning, and robust cross-validation.\n",
    "\n",
    "### Placeholder: PR curves / Ablation\n",
    "- TODO: When ground-truth labels are available, compute per-class PR curves.\n",
    "- TODO: Render an ablation summary comparing baseline vs extended features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}